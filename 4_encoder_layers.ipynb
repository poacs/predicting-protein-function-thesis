{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8005eec9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in ./.venv/lib/python3.12/site-packages (2.7.0)\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.12/site-packages (4.51.3)\n",
      "Requirement already satisfied: fair-esm in ./.venv/lib/python3.12/site-packages (2.0.0)\n",
      "Requirement already satisfied: biopython in ./.venv/lib/python3.12/site-packages (1.85)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: pandas in ./.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in ./.venv/lib/python3.12/site-packages (from torch) (4.13.2)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.12/site-packages (from torch) (79.0.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.12/site-packages (from torch) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in ./.venv/lib/python3.12/site-packages (from torch) (2025.3.2)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in ./.venv/lib/python3.12/site-packages (from torch) (9.5.1.17)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in ./.venv/lib/python3.12/site-packages (from torch) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in ./.venv/lib/python3.12/site-packages (from torch) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in ./.venv/lib/python3.12/site-packages (from torch) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in ./.venv/lib/python3.12/site-packages (from torch) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in ./.venv/lib/python3.12/site-packages (from torch) (0.6.3)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in ./.venv/lib/python3.12/site-packages (from torch) (2.26.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in ./.venv/lib/python3.12/site-packages (from torch) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in ./.venv/lib/python3.12/site-packages (from torch) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.3.0 in ./.venv/lib/python3.12/site-packages (from torch) (3.3.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in ./.venv/lib/python3.12/site-packages (from transformers) (0.30.2)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.12/site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: scipy>=1.6.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in ./.venv/lib/python3.12/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "True\n",
      "Tesla T4\n"
     ]
    }
   ],
   "source": [
    "# with heavier dropout & LayerNorm, gelu activation, higher dim_feedforward, LR scheduler, weight decay, gradient clipping\n",
    "!pip3 install torch transformers fair-esm biopython scikit-learn pandas\n",
    "\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(torch.cuda.is_available())  # should return true\n",
    "print(torch.cuda.get_device_name(0))  # should say T4\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from collections import Counter\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import ast  # parses string representations\n",
    "\n",
    "# imports for evaluation\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix\n",
    ")\n",
    "\n",
    "class SegmentedDataset(Dataset):\n",
    "    def __init__(self, embeddings_path, csv_path):\n",
    "        # embeddings_path: path to .npy file with a dict mapping protein IDs to lists of segment embeddings\n",
    "        # csv_path: path to CSV file with filtered labels\n",
    "        \n",
    "        # load the embeddings dictionary\n",
    "        self.embeddings_dict = np.load(embeddings_path, allow_pickle=True).item()\n",
    "        \n",
    "        # Load CSV with GO annotations\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        \n",
    "        # keep only rows where Entry exists in embeddings and has at least one segment\n",
    "        self.df = self.df[self.df[\"Entry\"].isin(self.embeddings_dict)]\n",
    "        self.df = self.df[self.df[\"Entry\"].apply(lambda x: len(self.embeddings_dict[x]) > 0)]\n",
    "        \n",
    "        # ensure the molecular_functions column is parsed correctly\n",
    "        def parse_str(s):\n",
    "            if isinstance(s, str):\n",
    "                try:\n",
    "                    return ast.literal_eval(s)\n",
    "                except Exception:\n",
    "                    return [item.strip() for item in s.split(';')]\n",
    "            return []\n",
    "        \n",
    "        self.df[\"molecular_functions\"] = self.df[\"molecular_functions\"].apply(parse_str)\n",
    "        self.entries = self.df[\"Entry\"].tolist()\n",
    "\n",
    "        # create multi-label binarized labels\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.labels = self.mlb.fit_transform(self.df[\"molecular_functions\"])\n",
    "        # self.num_classes = self.labels.shape[1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.entries[idx]\n",
    "        seg_list = self.embeddings_dict[entry]\n",
    "        seg_tensors = [torch.tensor(seg, dtype=torch.float) for seg in seg_list]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return seg_tensors, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # this collate function pads sequences of segment embeddings for each protein\n",
    "    seg_lists, labels = zip(*batch)\n",
    "\n",
    "    # pad to max segments\n",
    "    padded = pad_sequence([torch.stack(seg) for seg in seg_lists], batch_first=True)  # (B, S_max, emb_dim)\n",
    "    labels = torch.stack(labels)\n",
    "\n",
    "    # mask: true where padding\n",
    "    B, S, _ = padded.shape\n",
    "    mask = torch.zeros(B, S, dtype=torch.bool)\n",
    "\n",
    "    for i, seg in enumerate(seg_lists):\n",
    "        if len(seg)<S:\n",
    "            mask[i, len(seg):] = True\n",
    "    return padded, labels, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d55fe89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, nhead=4, num_layers=4, dim_feedforward=512, dropout=0.3, num_classes=50):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "\n",
    "        d_model = input_dim\n",
    "\n",
    "        # stack of transformer encoders\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=False, activation='gelu')\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, pad_mask):\n",
    "        x = x.transpose(0,1) # (S, B, d_model)\n",
    "        x = self.encoder(x, src_key_padding_mask=pad_mask)\n",
    "        x = x.transpose(0,1) # (B, S, d_model)\n",
    "\n",
    "        # mask out padding then mean-pool\n",
    "        mask = (~pad_mask).unsqueeze(-1) # (B, S, 1)\n",
    "        x = x * mask\n",
    "        x = x.sum(dim=1) / mask.sum(dim=1) # (B, d_model)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return self.classifier(x)\n",
    "    \n",
    "def train_model(model, dataloader, criterion, optimizer, scheduler, device, num_epochs=10, clip_norm=1.0):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        y_true_all = []\n",
    "        y_pred_all = []\n",
    "\n",
    "        for batch in dataloader:\n",
    "            inputs, labels, pad_mask = batch\n",
    "\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pad_mask = pad_mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, pad_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "\n",
    "            # gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            # collect for metrics\n",
    "            with torch.no_grad():\n",
    "                probs = torch.sigmoid(outputs)\n",
    "                preds = (probs > 0.5).long()\n",
    "                y_true_all.append(labels.cpu().numpy())\n",
    "                y_pred_all.append(preds.cpu().numpy())\n",
    "        \n",
    "        scheduler.step()\n",
    "            \n",
    "        avg_loss = epoch_loss / len(dataloader.dataset)\n",
    "\n",
    "        y_true = np.vstack(y_true_all)\n",
    "        y_pred = np.vstack(y_pred_all)\n",
    "\n",
    "        acc = (y_true == y_pred).all(axis=1).mean()\n",
    "\n",
    "        f1 = f1_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "        precision = precision_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "        recall = recall_score(y_true, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "        # flatten for sklearn's balanced accuracy, each class instance as one sample\n",
    "        balanced_acc = balanced_accuracy_score(y_true.flatten(), y_pred.flatten())\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Subset Accuracy: {acc:.4f}\")\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {avg_loss:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f} | Balanced Acc: {balanced_acc:.4f}\")\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "def evaluate_model(model, dataloader, device, threshold=0.5):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, pad_mask in dataloader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            pad_mask = pad_mask.to(device)\n",
    "\n",
    "            outputs = model(inputs, pad_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > threshold).long()\n",
    "\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "\n",
    "    y_true = np.vstack(all_labels)\n",
    "    y_pred = np.vstack(all_preds)\n",
    "\n",
    "    # confusion matrices for first 10 GO classes\n",
    "    classes = dataloader.dataset.mlb.classes_\n",
    "    for i in range(min(10, y_true.shape[1])):\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true[:,i], y_pred[:,i]).ravel()\n",
    "        print(f\"[{i:2d}] {classes[i]}\")\n",
    "        print(f\"    TN={tn:5d}  FP={fp:5d}  FN={fn:5d}  TP={tp:5d}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691ab40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 10540 proteins\n",
      "Number of classes: 50\n",
      "Using GPU: Tesla T4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/predicting-protein-function-thesis/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is on: cuda:0\n",
      "Epoch 1/10 | Subset Accuracy: 0.0230\n",
      "Epoch 1/10 | Loss: 0.7953 | F1: 0.3235 | Precision: 0.2199 | Recall: 0.7721 | Balanced Acc: 0.7783\n",
      "Epoch 2/10 | Subset Accuracy: 0.0486\n",
      "Epoch 2/10 | Loss: 0.5720 | F1: 0.4828 | Precision: 0.3683 | Recall: 0.8601 | Balanced Acc: 0.8539\n",
      "Epoch 3/10 | Subset Accuracy: 0.0577\n",
      "Epoch 3/10 | Loss: 0.5237 | F1: 0.5107 | Precision: 0.3965 | Recall: 0.8729 | Balanced Acc: 0.8691\n",
      "Epoch 4/10 | Subset Accuracy: 0.0664\n",
      "Epoch 4/10 | Loss: 0.4807 | F1: 0.5356 | Precision: 0.4206 | Recall: 0.8890 | Balanced Acc: 0.8817\n",
      "Epoch 5/10 | Subset Accuracy: 0.0748\n",
      "Epoch 5/10 | Loss: 0.4386 | F1: 0.5599 | Precision: 0.4475 | Recall: 0.8965 | Balanced Acc: 0.8912\n",
      "Epoch 6/10 | Subset Accuracy: 0.0894\n",
      "Epoch 6/10 | Loss: 0.3971 | F1: 0.5821 | Precision: 0.4693 | Recall: 0.9070 | Balanced Acc: 0.9011\n",
      "Epoch 7/10 | Subset Accuracy: 0.0982\n",
      "Epoch 7/10 | Loss: 0.3706 | F1: 0.6018 | Precision: 0.4913 | Recall: 0.9154 | Balanced Acc: 0.9090\n",
      "Epoch 8/10 | Subset Accuracy: 0.1094\n",
      "Epoch 8/10 | Loss: 0.3381 | F1: 0.6253 | Precision: 0.5197 | Recall: 0.9228 | Balanced Acc: 0.9150\n",
      "Epoch 9/10 | Subset Accuracy: 0.1147\n",
      "Epoch 9/10 | Loss: 0.3138 | F1: 0.6381 | Precision: 0.5315 | Recall: 0.9353 | Balanced Acc: 0.9220\n",
      "Epoch 10/10 | Subset Accuracy: 0.1258\n",
      "Epoch 10/10 | Loss: 0.2996 | F1: 0.6474 | Precision: 0.5402 | Recall: 0.9411 | Balanced Acc: 0.9255\n",
      "[ 0] 3-phosphoinositide-dependent protein kinase activity [GO:0004676]\n",
      "    TN=10262  FP=   33  FN=    5  TP=  240\n",
      "\n",
      "[ 1] AMP-activated protein kinase activity [GO:0004679]\n",
      "    TN=10257  FP=   36  FN=    4  TP=  243\n",
      "\n",
      "[ 2] ATP binding [GO:0005524]\n",
      "    TN= 8820  FP=  243  FN=   41  TP= 1436\n",
      "\n",
      "[ 3] ATP hydrolysis activity [GO:0016887]\n",
      "    TN= 9959  FP=  164  FN=    6  TP=  411\n",
      "\n",
      "[ 4] DNA binding [GO:0003677]\n",
      "    TN= 7527  FP= 2083  FN=  125  TP=  805\n",
      "\n",
      "[ 5] DNA-binding transcription activator activity, RNA polymerase II-specific [GO:0001228]\n",
      "    TN= 9127  FP=  938  FN=    5  TP=  470\n",
      "\n",
      "[ 6] DNA-binding transcription factor activity [GO:0003700]\n",
      "    TN= 9044  FP= 1010  FN=    4  TP=  482\n",
      "\n",
      "[ 7] DNA-binding transcription factor activity, RNA polymerase II-specific [GO:0000981]\n",
      "    TN= 9046  FP=  254  FN=   12  TP= 1228\n",
      "\n",
      "[ 8] DNA-binding transcription repressor activity, RNA polymerase II-specific [GO:0001227]\n",
      "    TN= 9310  FP=  964  FN=    7  TP=  259\n",
      "\n",
      "[ 9] DNA-dependent protein kinase activity [GO:0004677]\n",
      "    TN=10261  FP=   33  FN=    6  TP=  240\n",
      "\n",
      "Model saved as protein_transformer_model_4layer_reg.pt\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # set file paths\n",
    "    embeddings_path = \"Data/esm2_segmented_embeddings.npy\"\n",
    "    csv_path = \"Data/filtered_parsed_data.csv\"\n",
    "    \n",
    "    # create dataset and dataloader\n",
    "    dataset = SegmentedDataset(embeddings_path, csv_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn, num_workers=0)\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataset)} proteins\")\n",
    "    print(f\"Number of classes: {dataset.labels.shape[1]}\")\n",
    "    \n",
    "    # get input dimension from one sample\n",
    "    sample_segments, _ = dataset[0]\n",
    "    input_dim = sample_segments[0].shape[0]\n",
    "    \n",
    "    # instantiate transformer classifier model\n",
    "    model = TransformerClassifier(input_dim=input_dim, nhead=4, num_layers=4, dim_feedforward=512, dropout=0.3, num_classes=dataset.labels.shape[1])\n",
    "    \n",
    "    # use CUDA if available, AWS EC2 with GPU\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    print(f\"model is on: {next(model.parameters()).device}\")\n",
    "\n",
    "    # set up loss function and optimizer for multi-label classification\n",
    "    # compute class weights\n",
    "    label_counts = dataset.labels.sum(axis=0)  # positive counts per class\n",
    "    total_counts = len(dataset)  # total samples\n",
    "\n",
    "    # avoid division by zero\n",
    "    pos_weights = (total_counts - label_counts) / (label_counts + 1e-5)\n",
    "    class_weights_tensor = torch.tensor(pos_weights, dtype=torch.float).to(device)\n",
    "\n",
    "    # weighted loss\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=5e-5,\n",
    "        weight_decay=1e-4 # weight decay for regularization\n",
    "    )\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=10, # cycle length = num_epochs\n",
    "        eta_min=1e-6\n",
    "    )\n",
    "    \n",
    "    # train the model\n",
    "    train_model(model, dataloader, criterion, optimizer, scheduler, device, num_epochs=10)\n",
    "    evaluate_model(model, dataloader, device)\n",
    "    \n",
    "    # save the trained model state\n",
    "    torch.save(model.state_dict(), \"protein_transformer_model_4layer_reg.pt\")\n",
    "    print(\"Model saved as protein_transformer_model_4layer_reg.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
