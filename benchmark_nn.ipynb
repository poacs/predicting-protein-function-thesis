{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GPU: Tesla T4\n",
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast  # for parsing string representations\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    multilabel_confusion_matrix\n",
    ")\n",
    "\n",
    "# use CUDA if available, for AWS EC2 with GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "print(\"using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinDataset(Dataset):\n",
    "    def __init__(self, embeddings_path, csv_path):\n",
    "        # embeddings_path is the path to the .npy file with dict mapping protein IDs to lists of segment embeddings\n",
    "        # csv_path is the path to the .csv file with filtered labels\n",
    "        self.embeddings_dict = np.load(embeddings_path, allow_pickle=True).item()\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        # keep only proteins that exist in the embeddings and have at least one segment\n",
    "        self.df = self.df[self.df[\"Entry\"].isin(self.embeddings_dict.keys())].reset_index(drop=True)\n",
    "        self.df = self.df[self.df[\"Entry\"].apply(lambda x: len(self.embeddings_dict[x]) > 0)]\n",
    "        self.entries = self.df[\"Entry\"].tolist()\n",
    "        \n",
    "        # ensure molecular_functions column is parsed into lists\n",
    "        def parse_str(s):\n",
    "            if isinstance(s, str):\n",
    "                try:\n",
    "                    return ast.literal_eval(s)\n",
    "                except Exception:\n",
    "                    return [item.strip() for item in s.split(';') if item.strip()]\n",
    "            return s\n",
    "        self.df[\"molecular_functions\"] = self.df[\"molecular_functions\"].apply(parse_str)\n",
    "        \n",
    "        # binarize labels\n",
    "        from sklearn.preprocessing import MultiLabelBinarizer\n",
    "        self.mlb = MultiLabelBinarizer()\n",
    "        self.labels = self.mlb.fit_transform(self.df[\"molecular_functions\"])\n",
    "        self.num_classes = self.labels.shape[1]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.entries)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        entry = self.entries[idx]\n",
    "        seg_list = self.embeddings_dict[entry]  # list of segment embeddings (numpy arrays)\n",
    "        # convert each segment to a torch tensor\n",
    "        seg_tensors = [torch.tensor(seg, dtype=torch.float) for seg in seg_list]\n",
    "        label = torch.tensor(self.labels[idx], dtype=torch.float)\n",
    "        return seg_tensors, label\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # collate function to pad variable number of segments per protein.\n",
    "    all_segments = []\n",
    "    labels = []\n",
    "    lengths = []\n",
    "\n",
    "    for seg_list, label in batch:\n",
    "        lengths.append(len(seg_list))\n",
    "        # stack segments of one protein: shape (num_segments, embedding_dim)\n",
    "        all_segments.append(torch.stack(seg_list))\n",
    "        labels.append(label)\n",
    "    # pad sequences along the segments axis so that all proteins have the same number of segments\n",
    "    padded_segments = pad_sequence(all_segments, batch_first=True)  # (batch_size, max_seq_len, embedding_dim)\n",
    "    labels = torch.stack(labels)  # (batch_size, num_classes)\n",
    "    batch_size, max_seq_len, _ = padded_segments.size()\n",
    "    pad_mask = torch.zeros((batch_size, max_seq_len), dtype=torch.bool)\n",
    "    \n",
    "    for i, l in enumerate(lengths):\n",
    "        if l < max_seq_len:\n",
    "            pad_mask[i, l:] = True\n",
    "    return padded_segments, labels, pad_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaselineProteinClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, hidden_dim=512, hidden_dim2=256):\n",
    "        # input_dim is the dimension of each segment embedding (like 1280 for esm2_t33_650M_UR50D)\n",
    "        # num_classes is the number of output classes (from MultiLabelBinarizer)\n",
    "        super(BaselineProteinClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim2)\n",
    "        self.fc3 = nn.Linear(hidden_dim2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        # x: (batch_size, seq_len, input_dim) containing segment embeddings\n",
    "        # src_key_padding_mask: boolean mask of shape (batch_size, seq_len) marking padded segments\n",
    "        # mean pool over the sequence of segments, ignoring padded positions if mask is provided\n",
    "        if src_key_padding_mask is not None:\n",
    "            mask = (~src_key_padding_mask).unsqueeze(-1).float()  # shape: (batch_size, seq_len, 1)\n",
    "            x = x * mask\n",
    "            x = x.sum(dim=1) / mask.sum(dim=1)\n",
    "        else:\n",
    "            x = x.mean(dim=1)  # (batch_size, input_dim)\n",
    "        \n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        logits = self.fc3(x)\n",
    "        return logits\n",
    "\n",
    "# for training, we're going to use BCEWithLogitsLoss for multi-label classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0: 3-phosphoinositide-dependent protein kinase activity [GO:0004676]\n",
      "  1: AMP-activated protein kinase activity [GO:0004679]\n",
      "  2: ATP binding [GO:0005524]\n",
      "  3: ATP hydrolysis activity [GO:0016887]\n",
      "  4: DNA binding [GO:0003677]\n",
      "  5: DNA-binding transcription activator activity, RNA polymerase II-specific [GO:0001228]\n",
      "  6: DNA-binding transcription factor activity [GO:0003700]\n",
      "  7: DNA-binding transcription factor activity, RNA polymerase II-specific [GO:0000981]\n",
      "  8: DNA-binding transcription repressor activity, RNA polymerase II-specific [GO:0001227]\n",
      "  9: DNA-dependent protein kinase activity [GO:0004677]\n",
      " 10: G protein-coupled receptor activity [GO:0004930]\n",
      " 11: GTP binding [GO:0005525]\n",
      " 12: GTPase activator activity [GO:0005096]\n",
      " 13: GTPase activity [GO:0003924]\n",
      " 14: RNA binding [GO:0003723]\n",
      " 15: RNA polymerase II cis-regulatory region sequence-specific DNA binding [GO:0000978]\n",
      " 16: RNA polymerase II transcription regulatory region sequence-specific DNA binding [GO:0000977]\n",
      " 17: actin binding [GO:0003779]\n",
      " 18: cadherin binding [GO:0045296]\n",
      " 19: calcium ion binding [GO:0005509]\n",
      " 20: chromatin binding [GO:0003682]\n",
      " 21: enzyme binding [GO:0019899]\n",
      " 22: eukaryotic translation initiation factor 2alpha kinase activity [GO:0004694]\n",
      " 23: histone H2AS1 kinase activity [GO:0044024]\n",
      " 24: histone H2AS121 kinase activity [GO:0072371]\n",
      " 25: histone H2AT120 kinase activity [GO:1990244]\n",
      " 26: histone H2AXS139 kinase activity [GO:0035979]\n",
      " 27: histone H2BS14 kinase activity [GO:0044025]\n",
      " 28: histone H2BS36 kinase activity [GO:0140823]\n",
      " 29: histone H3T11 kinase activity [GO:0035402]\n",
      " 30: histone H3T45 kinase activity [GO:0140857]\n",
      " 31: histone H3T6 kinase activity [GO:0035403]\n",
      " 32: identical protein binding [GO:0042802]\n",
      " 33: metal ion binding [GO:0046872]\n",
      " 34: microtubule binding [GO:0008017]\n",
      " 35: olfactory receptor activity [GO:0004984]\n",
      " 36: protein heterodimerization activity [GO:0046982]\n",
      " 37: protein homodimerization activity [GO:0042803]\n",
      " 38: protein kinase binding [GO:0019901]\n",
      " 39: protein serine kinase activity [GO:0106310]\n",
      " 40: protein serine/threonine kinase activity [GO:0004674]\n",
      " 41: protein-containing complex binding [GO:0044877]\n",
      " 42: sequence-specific DNA binding [GO:0043565]\n",
      " 43: sequence-specific double-stranded DNA binding [GO:1990837]\n",
      " 44: signaling receptor binding [GO:0005102]\n",
      " 45: small GTPase binding [GO:0031267]\n",
      " 46: transcription coactivator activity [GO:0003713]\n",
      " 47: ubiquitin protein ligase activity [GO:0061630]\n",
      " 48: ubiquitin protein ligase binding [GO:0031625]\n",
      " 49: zinc ion binding [GO:0008270]\n",
      "Dataset size: 10540 proteins\n",
      "Number of classes: 50\n",
      "Epoch 1/10 — Loss: 0.2140  Acc: 0.0454  Prec: 0.0733  Rec: 0.0394  F1: 0.0454 Balanced Acc: 0.9417\n",
      "Class  0 ── TN: 10295  FP:     0  FN:   245  TP:     0\n",
      "Class  1 ── TN: 10293  FP:     0  FN:   247  TP:     0\n",
      "Class  2 ── TN:  9063  FP:     0  FN:  1477  TP:     0\n",
      "Class  3 ── TN: 10123  FP:     0  FN:   417  TP:     0\n",
      "Class  4 ── TN:  9610  FP:     0  FN:   930  TP:     0\n",
      "Class  5 ── TN: 10065  FP:     0  FN:   475  TP:     0\n",
      "Class  6 ── TN: 10054  FP:     0  FN:   486  TP:     0\n",
      "Class  7 ── TN:  9220  FP:    80  FN:  1004  TP:   236\n",
      "Class  8 ── TN: 10274  FP:     0  FN:   266  TP:     0\n",
      "Class  9 ── TN: 10294  FP:     0  FN:   246  TP:     0\n",
      "Epoch 2/10 — Loss: 0.1447  Acc: 0.0794  Prec: 0.1812  Rec: 0.0834  F1: 0.0982 Balanced Acc: 0.8898\n",
      "Class  0 ── TN: 10295  FP:     0  FN:   245  TP:     0\n",
      "Class  1 ── TN: 10293  FP:     0  FN:   247  TP:     0\n",
      "Class  2 ── TN:  9022  FP:    41  FN:  1232  TP:   245\n",
      "Class  3 ── TN: 10123  FP:     0  FN:   417  TP:     0\n",
      "Class  4 ── TN:  9610  FP:     0  FN:   930  TP:     0\n",
      "Class  5 ── TN: 10055  FP:    10  FN:   459  TP:    16\n",
      "Class  6 ── TN: 10054  FP:     0  FN:   486  TP:     0\n",
      "Class  7 ── TN:  9017  FP:   283  FN:   357  TP:   883\n",
      "Class  8 ── TN: 10274  FP:     0  FN:   266  TP:     0\n",
      "Class  9 ── TN: 10294  FP:     0  FN:   246  TP:     0\n",
      "Epoch 3/10 — Loss: 0.1282  Acc: 0.0967  Prec: 0.3405  Rec: 0.2100  F1: 0.2450 Balanced Acc: 0.8543\n",
      "Class  0 ── TN: 10240  FP:    55  FN:   167  TP:    78\n",
      "Class  1 ── TN: 10223  FP:    70  FN:   154  TP:    93\n",
      "Class  2 ── TN:  8912  FP:   151  FN:  1011  TP:   466\n",
      "Class  3 ── TN: 10123  FP:     0  FN:   417  TP:     0\n",
      "Class  4 ── TN:  9610  FP:     0  FN:   930  TP:     0\n",
      "Class  5 ── TN: 10060  FP:     5  FN:   469  TP:     6\n",
      "Class  6 ── TN: 10034  FP:    20  FN:   476  TP:    10\n",
      "Class  7 ── TN:  9029  FP:   271  FN:   267  TP:   973\n",
      "Class  8 ── TN: 10274  FP:     0  FN:   266  TP:     0\n",
      "Class  9 ── TN: 10224  FP:    70  FN:   152  TP:    94\n",
      "Epoch 4/10 — Loss: 0.1197  Acc: 0.1092  Prec: 0.4254  Rec: 0.2264  F1: 0.2743 Balanced Acc: 0.8659\n",
      "Class  0 ── TN: 10242  FP:    53  FN:   169  TP:    76\n",
      "Class  1 ── TN: 10231  FP:    62  FN:   155  TP:    92\n",
      "Class  2 ── TN:  9006  FP:    57  FN:   980  TP:   497\n",
      "Class  3 ── TN: 10123  FP:     0  FN:   417  TP:     0\n",
      "Class  4 ── TN:  9610  FP:     0  FN:   930  TP:     0\n",
      "Class  5 ── TN:  9980  FP:    85  FN:   374  TP:   101\n",
      "Class  6 ── TN: 10017  FP:    37  FN:   465  TP:    21\n",
      "Class  7 ── TN:  9044  FP:   256  FN:   250  TP:   990\n",
      "Class  8 ── TN: 10274  FP:     0  FN:   266  TP:     0\n",
      "Class  9 ── TN: 10235  FP:    59  FN:   158  TP:    88\n",
      "Epoch 5/10 — Loss: 0.1145  Acc: 0.1301  Prec: 0.4086  Rec: 0.2885  F1: 0.3220 Balanced Acc: 0.8630\n",
      "Class  0 ── TN: 10205  FP:    90  FN:   116  TP:   129\n",
      "Class  1 ── TN: 10209  FP:    84  FN:   120  TP:   127\n",
      "Class  2 ── TN:  8977  FP:    86  FN:   851  TP:   626\n",
      "Class  3 ── TN: 10123  FP:     0  FN:   417  TP:     0\n",
      "Class  4 ── TN:  9610  FP:     0  FN:   930  TP:     0\n",
      "Class  5 ── TN:  9984  FP:    81  FN:   366  TP:   109\n",
      "Class  6 ── TN: 10008  FP:    46  FN:   461  TP:    25\n",
      "Class  7 ── TN:  9038  FP:   262  FN:   192  TP:  1048\n",
      "Class  8 ── TN: 10274  FP:     0  FN:   266  TP:     0\n",
      "Class  9 ── TN: 10209  FP:    85  FN:   120  TP:   126\n",
      "Epoch 6/10 — Loss: 0.1109  Acc: 0.1444  Prec: 0.4446  Rec: 0.3483  F1: 0.3577 Balanced Acc: 0.8637\n",
      "Class  0 ── TN: 10188  FP:   107  FN:    70  TP:   175\n",
      "Class  1 ── TN: 10189  FP:   104  FN:    80  TP:   167\n",
      "Class  2 ── TN:  8887  FP:   176  FN:   659  TP:   818\n",
      "Class  3 ── TN: 10123  FP:     0  FN:   417  TP:     0\n",
      "Class  4 ── TN:  9610  FP:     0  FN:   930  TP:     0\n",
      "Class  5 ── TN: 10035  FP:    30  FN:   416  TP:    59\n",
      "Class  6 ── TN: 10030  FP:    24  FN:   473  TP:    13\n",
      "Class  7 ── TN:  9054  FP:   246  FN:   196  TP:  1044\n",
      "Class  8 ── TN: 10274  FP:     0  FN:   266  TP:     0\n",
      "Class  9 ── TN: 10189  FP:   105  FN:    76  TP:   170\n",
      "Epoch 7/10 — Loss: 0.1078  Acc: 0.1591  Prec: 0.4928  Rec: 0.2977  F1: 0.3486 Balanced Acc: 0.8853\n",
      "Class  0 ── TN: 10245  FP:    50  FN:   128  TP:   117\n",
      "Class  1 ── TN: 10245  FP:    48  FN:   129  TP:   118\n",
      "Class  2 ── TN:  8888  FP:   175  FN:   586  TP:   891\n",
      "Class  3 ── TN: 10108  FP:    15  FN:   326  TP:    91\n",
      "Class  4 ── TN:  9610  FP:     0  FN:   930  TP:     0\n",
      "Class  5 ── TN: 10038  FP:    27  FN:   427  TP:    48\n",
      "Class  6 ── TN: 10033  FP:    21  FN:   474  TP:    12\n",
      "Class  7 ── TN:  9076  FP:   224  FN:   239  TP:  1001\n",
      "Class  8 ── TN: 10274  FP:     0  FN:   266  TP:     0\n",
      "Class  9 ── TN: 10240  FP:    54  FN:   125  TP:   121\n",
      "Epoch 8/10 — Loss: 0.1052  Acc: 0.1779  Prec: 0.5347  Rec: 0.3733  F1: 0.3965 Balanced Acc: 0.8733\n",
      "Class  0 ── TN: 10204  FP:    91  FN:    73  TP:   172\n",
      "Class  1 ── TN: 10206  FP:    87  FN:    77  TP:   170\n",
      "Class  2 ── TN:  8928  FP:   135  FN:   568  TP:   909\n",
      "Class  3 ── TN: 10105  FP:    18  FN:   292  TP:   125\n",
      "Class  4 ── TN:  9607  FP:     3  FN:   922  TP:     8\n",
      "Class  5 ── TN:  9957  FP:   108  FN:   326  TP:   149\n",
      "Class  6 ── TN: 10021  FP:    33  FN:   463  TP:    23\n",
      "Class  7 ── TN:  9068  FP:   232  FN:   181  TP:  1059\n",
      "Class  8 ── TN: 10274  FP:     0  FN:   266  TP:     0\n",
      "Class  9 ── TN: 10205  FP:    89  FN:    76  TP:   170\n",
      "Epoch 9/10 — Loss: 0.1029  Acc: 0.1721  Prec: 0.5525  Rec: 0.3721  F1: 0.4088 Balanced Acc: 0.8863\n",
      "Class  0 ── TN: 10227  FP:    68  FN:    78  TP:   167\n",
      "Class  1 ── TN: 10219  FP:    74  FN:    80  TP:   167\n",
      "Class  2 ── TN:  8947  FP:   116  FN:   549  TP:   928\n",
      "Class  3 ── TN: 10110  FP:    13  FN:   272  TP:   145\n",
      "Class  4 ── TN:  9596  FP:    14  FN:   887  TP:    43\n",
      "Class  5 ── TN: 10011  FP:    54  FN:   357  TP:   118\n",
      "Class  6 ── TN:  9966  FP:    88  FN:   416  TP:    70\n",
      "Class  7 ── TN:  9074  FP:   226  FN:   179  TP:  1061\n",
      "Class  8 ── TN: 10274  FP:     0  FN:   266  TP:     0\n",
      "Class  9 ── TN: 10226  FP:    68  FN:    83  TP:   163\n",
      "Epoch 10/10 — Loss: 0.1008  Acc: 0.1764  Prec: 0.5675  Rec: 0.3180  F1: 0.3782 Balanced Acc: 0.8979\n",
      "Class  0 ── TN: 10261  FP:    34  FN:   128  TP:   117\n",
      "Class  1 ── TN: 10264  FP:    29  FN:   143  TP:   104\n",
      "Class  2 ── TN:  8961  FP:   102  FN:   529  TP:   948\n",
      "Class  3 ── TN: 10118  FP:     5  FN:   284  TP:   133\n",
      "Class  4 ── TN:  9610  FP:     0  FN:   929  TP:     1\n",
      "Class  5 ── TN: 10000  FP:    65  FN:   348  TP:   127\n",
      "Class  6 ── TN: 10037  FP:    17  FN:   471  TP:    15\n",
      "Class  7 ── TN:  9067  FP:   233  FN:   157  TP:  1083\n",
      "Class  8 ── TN: 10274  FP:     0  FN:   266  TP:     0\n",
      "Class  9 ── TN: 10266  FP:    28  FN:   136  TP:   110\n",
      "Model saved as protein_model.pt\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, dataloader, criterion, optimizer, device, num_epochs=10):\n",
    "    for epoch in range(1,num_epochs+1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for inputs, labels, pad_mask in dataloader:\n",
    "            inputs, labels, pad_mask = inputs.to(device), labels.to(device), pad_mask.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, src_key_padding_mask=pad_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        avg_loss = epoch_loss / len(dataloader.dataset)\n",
    "        \n",
    "        # print training results for the epoch\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels, pad_mask in dataloader:\n",
    "                inputs, labels, pad_mask = inputs.to(device), labels.to(device), pad_mask.to(device)\n",
    "                logits = model(inputs, src_key_padding_mask=pad_mask)\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs > 0.5).cpu().numpy()\n",
    "                all_preds.append(preds)\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "\n",
    "        all_preds = np.vstack(all_preds)\n",
    "        all_labels = np.vstack(all_labels)\n",
    "\n",
    "        balanced_acc = balanced_accuracy_score(all_preds.flatten(), all_labels.flatten())\n",
    "\n",
    "        # calculate metrics\n",
    "        acc   = accuracy_score(all_labels, all_preds)\n",
    "        prec  = precision_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        rec   = recall_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        f1    = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "        cms   = multilabel_confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs} — Loss: {avg_loss:.4f}\", end=\"  \")\n",
    "        print(f\"Acc: {acc:.4f}  Prec: {prec:.4f}  Rec: {rec:.4f}  F1: {f1:.4f} Balanced Acc: {balanced_acc:.4f}\")\n",
    "        \n",
    "        # print out the matrices for the first 10 classes\n",
    "        for idx, cm in enumerate(cms[:10]):\n",
    "            tn, fp, fn, tp = cm.ravel()\n",
    "            print(f\"Class {idx:2d} ── TN: {tn:5d}  FP: {fp:5d}  FN: {fn:5d}  TP: {tp:5d}\")\n",
    "\n",
    "# main training routine\n",
    "def main():\n",
    "    embeddings_path = \"Data/esm2_segmented_embeddings.npy\"\n",
    "    csv_path = \"Data/filtered_parsed_data.csv\"\n",
    "    \n",
    "    # create dataset and dataloader\n",
    "    dataset = ProteinDataset(embeddings_path, csv_path)\n",
    "    all_terms = dataset.mlb.classes_\n",
    "\n",
    "    for idx, term in enumerate(all_terms):\n",
    "        print(f\"{idx:3d}: {term}\")\n",
    "        dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
    "    \n",
    "    print(f\"Dataset size: {len(dataset)} proteins\")\n",
    "    print(f\"Number of classes: {dataset.num_classes}\")\n",
    "    \n",
    "    # get input dimension from one sample (assuming at least one segment exists)\n",
    "    sample_segments, _ = dataset[0]\n",
    "    input_dim = sample_segments[0].shape[0]\n",
    "    \n",
    "    # instantiate the baseline model\n",
    "    model = BaselineProteinClassifier(input_dim=input_dim, num_classes=dataset.num_classes)\n",
    "    model.to(device)\n",
    "    \n",
    "    # set up loss function and optimizer\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    # train model\n",
    "    train_model(model, dataloader, criterion, optimizer, device, num_epochs=10)\n",
    "    \n",
    "    # save the trained model\n",
    "    torch.save(model.state_dict(), \"protein_model.pt\")\n",
    "    print(\"Model saved as protein_model.pt\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
